{"cells": [{"cell_type": "markdown", "id": "fe017c4c-16ad-404f-92fd-6404fe27a629", "metadata": {}, "source": "# Introduction"}, {"cell_type": "markdown", "id": "a26e0605-1c98-4153-b57a-173c6e4795e6", "metadata": {}, "source": "BLANK"}, {"cell_type": "markdown", "id": "617a95af-2c70-4683-82a5-e7ecc4886fdd", "metadata": {}, "source": "# Converting Our Files to Parquet"}, {"cell_type": "code", "execution_count": 3, "id": "72746b97-a6ac-448e-a395-5dcd18f4668c", "metadata": {}, "outputs": [{"data": {"text/plain": "'\\nfnames = !gsutil ls gs://final_proj_bucket/VM2Uniform/*.zip\\nfor one in fnames:\\n    fullpath = one                # gs://pstat135-voter-file/VM2Uniform/VM2Uniform--NY--2021-03-15.zip \\n    gcs_path = !dirname {one}     # [gs://pstat135-voter-file/VM2Uniform]\\n    gcs_path = gcs_path[0]        # gs://pstat135-voter-file/VM2Uniform\\n    filename = !basename {one}    # [VM2Uniform--NY--2021-03-15.zip]\\n    filename = filename[0]        # VM2Uniform--NY--2021-03-15.zip\\n    \\n    # fileroot: VM2Uniform--NY--2021-03-15\\n    # fileext : zip\\n    fileroot, fileext = filename.split(\\'.\\')   \\n\\n    print(\"##########################\")\\n    print(filename)\\n    print(gcs_path)\\n    print(fileroot, fileext)\\n    \\n    !gsutil -m cp {one} .                     # copy to local disk\\n    !unzip {filename}                         # unzip\\n    !gsutil -m cp {fileroot}* {gcs_path}/     # copy raw files to bucket\\n    # !echo \\'{fileroot}*\\'\\n    tabfile = gcs_path+\\'/\\'+fileroot+\\'.tab\\'    # full path to tab file\\n    pqpath = gcs_path+\\'/\\'+fileroot            # parquet file directory\\n    \\n    df = spark.read.csv(tabfile, header = True, sep=\\'\\t\\') # read tab file\\n    df.write.mode(\\'overwrite\\').parquet(pqpath)            # write data as parquet file\\n    \\n    !rm /{fileroot}*  \\n'"}, "execution_count": 3, "metadata": {}, "output_type": "execute_result"}], "source": "'''\nfnames = !gsutil ls gs://final_proj_bucket/VM2Uniform/*.zip\nfor one in fnames:\n    fullpath = one                # gs://pstat135-voter-file/VM2Uniform/VM2Uniform--NY--2021-03-15.zip \n    gcs_path = !dirname {one}     # [gs://pstat135-voter-file/VM2Uniform]\n    gcs_path = gcs_path[0]        # gs://pstat135-voter-file/VM2Uniform\n    filename = !basename {one}    # [VM2Uniform--NY--2021-03-15.zip]\n    filename = filename[0]        # VM2Uniform--NY--2021-03-15.zip\n    \n    # fileroot: VM2Uniform--NY--2021-03-15\n    # fileext : zip\n    fileroot, fileext = filename.split('.')   \n\n    print(\"##########################\")\n    print(filename)\n    print(gcs_path)\n    print(fileroot, fileext)\n    \n    !gsutil -m cp {one} .                     # copy to local disk\n    !unzip {filename}                         # unzip\n    !gsutil -m cp {fileroot}* {gcs_path}/     # copy raw files to bucket\n    # !echo '{fileroot}*'\n    tabfile = gcs_path+'/'+fileroot+'.tab'    # full path to tab file\n    pqpath = gcs_path+'/'+fileroot            # parquet file directory\n    \n    df = spark.read.csv(tabfile, header = True, sep='\\t') # read tab file\n    df.write.mode('overwrite').parquet(pqpath)            # write data as parquet file\n    \n    !rm /{fileroot}*  \n'''"}, {"cell_type": "markdown", "id": "81ddfe0b-e209-4992-9eb9-5858cd6e70c2", "metadata": {}, "source": "# Creating Dataframes from Parquet Files"}, {"cell_type": "code", "execution_count": 4, "id": "04e322f4-9f5e-4134-a631-14117fbe505f", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/02/22 00:34:22 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"}, {"ename": "KeyboardInterrupt", "evalue": "", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)", "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m bucket_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgs://pstat135-voter-file/VM2Uniform/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m AK_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbucket_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mVM2Uniform--AK--2021-02-03\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m CA_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mparquet(bucket_dir \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVM2Uniform--CA--2021-05-02\u001b[39m\u001b[38;5;124m'\u001b[39m)\n", "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/readwriter.py:458\u001b[0m, in \u001b[0;36mDataFrameReader.parquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    453\u001b[0m recursiveFileLookup \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecursiveFileLookup\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(mergeSchema\u001b[38;5;241m=\u001b[39mmergeSchema, pathGlobFilter\u001b[38;5;241m=\u001b[39mpathGlobFilter,\n\u001b[1;32m    455\u001b[0m                recursiveFileLookup\u001b[38;5;241m=\u001b[39mrecursiveFileLookup, modifiedBefore\u001b[38;5;241m=\u001b[39mmodifiedBefore,\n\u001b[1;32m    456\u001b[0m                modifiedAfter\u001b[38;5;241m=\u001b[39mmodifiedAfter)\n\u001b[0;32m--> 458\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n", "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1303\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1296\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1305\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n", "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1033\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1033\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1035\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n", "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1200\u001b[0m, in \u001b[0;36mGatewayConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JNetworkError(\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while sending\u001b[39m\u001b[38;5;124m\"\u001b[39m, e, proto\u001b[38;5;241m.\u001b[39mERROR_ON_SEND)\n\u001b[1;32m   1199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1200\u001b[0m     answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m   1201\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m   1202\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m answer\u001b[38;5;241m.\u001b[39mstartswith(proto\u001b[38;5;241m.\u001b[39mRETURN_MESSAGE):\n", "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n", "\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}, {"name": "stderr", "output_type": "stream", "text": "23/02/22 00:34:37 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/02/22 00:34:52 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"}], "source": "bucket_dir = 'gs://pstat135-voter-file/VM2Uniform/'\nAK_df = spark.read.parquet(bucket_dir + 'VM2Uniform--AK--2021-02-03')\nCA_df = spark.read.parquet(bucket_dir + 'VM2Uniform--CA--2021-05-02')"}, {"cell_type": "markdown", "id": "fdc47b8d-a3ee-4b6c-b0fd-2e654196e269", "metadata": {}, "source": "# Explorative Data Analysis (EDA)"}, {"cell_type": "markdown", "id": "3e153ffe-8eb0-4c4e-b474-c7ef2fe9bf1a", "metadata": {}, "source": "## Alaska Dataframe"}, {"cell_type": "markdown", "id": "4f4e4c84-6e0a-41bb-a157-96d0ba5399a0", "metadata": {}, "source": "### Dataframe Dimensions and Schema"}, {"cell_type": "markdown", "id": "8300240d-524d-4022-a7ce-295d9bfcec69", "metadata": {}, "source": "First let's look at the number of rows and columns in our dataframe..."}, {"cell_type": "code", "execution_count": 5, "id": "73cf2e72-dc48-4587-8848-6c9539ad261a", "metadata": {}, "outputs": [{"ename": "NameError", "evalue": "name 'AK_df' is not defined", "output_type": "error", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)", "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of Rows:    \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mAK_df\u001b[49m\u001b[38;5;241m.\u001b[39mcount(), \\\n\u001b[1;32m      2\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mNumber of Columns: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(AK_df\u001b[38;5;241m.\u001b[39mcolumns))\n", "\u001b[0;31mNameError\u001b[0m: name 'AK_df' is not defined"]}], "source": "print(\"Number of Rows:    \", AK_df.count(), \\\n      \"\\nNumber of Columns: \", len(AK_df.columns))"}, {"cell_type": "markdown", "id": "307f8809-3d62-4301-a049-faf68fb4220d", "metadata": {}, "source": "As we can see for our Alaska dataframe, we have $548,259$ rows and $726$ columns."}, {"cell_type": "markdown", "id": "9219c4ea-8428-4ab0-9843-3158e10082ca", "metadata": {}, "source": "Now let us look at all of the features (or column variables) we have available to us in our dataframe..."}, {"cell_type": "code", "execution_count": 6, "id": "35ffeb6d-bbbc-433c-8265-fe9bdf1fdb12", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/02/22 00:35:07 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/02/22 00:35:22 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/02/22 00:35:37 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/02/22 00:35:52 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/02/22 00:36:07 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/02/22 00:36:22 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/02/22 00:36:37 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/02/22 00:36:52 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/02/22 00:37:07 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/02/22 00:37:22 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/02/22 00:37:37 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/02/22 00:37:52 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/02/22 00:38:07 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/02/22 00:38:22 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/02/22 00:38:37 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/02/22 00:38:52 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/02/22 00:39:07 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/02/22 00:39:22 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/02/22 00:39:37 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/02/22 00:39:52 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/02/22 00:40:07 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/02/22 00:40:22 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/02/22 00:40:37 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/02/22 00:40:52 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/02/22 00:41:07 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/02/22 00:41:22 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/02/22 00:41:37 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/02/22 00:41:52 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/02/22 00:42:07 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/02/22 00:42:22 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/02/22 00:42:37 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/02/22 00:42:52 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/02/22 00:43:07 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n23/02/22 00:43:22 WARN org.apache.spark.scheduler.cluster.YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"}], "source": "#AK_df.printSchema()"}, {"cell_type": "markdown", "id": "0165c0ac-9a90-4dae-9151-619ff14e8365", "metadata": {}, "source": "We clearly have a lot of features (726 to be exact) to consider in this dataframe that we must further understand and explore prior to our feature selection and feature engineering phase."}, {"cell_type": "markdown", "id": "43daefa8-ab52-4275-9b44-017e6473071a", "metadata": {}, "source": "### Missing Value Handling"}, {"cell_type": "code", "execution_count": null, "id": "1386c0c3-efb2-48aa-b787-1c546c91cf79", "metadata": {}, "outputs": [], "source": "AK_cols_list = AK_df.columns"}, {"cell_type": "markdown", "id": "5bfdcc27-e16a-4d88-b61f-732802b71bd2", "metadata": {}, "source": "### Feature Analysis"}, {"cell_type": "markdown", "id": "7e484c01-9117-4488-9bbd-e40ddef24114", "metadata": {}, "source": "Now that we have some further insight on the feature contents of the Alaska Voter file, through the exploration of the dataframes dimensions and schema, we must now achieve a more in depth understanding of what each feature's data represents."}, {"cell_type": "markdown", "id": "9fce7969-ab5b-4236-82eb-38f15c36d729", "metadata": {}, "source": "## California Dataframe"}, {"cell_type": "code", "execution_count": 40, "id": "4d936ba4-63c6-4f12-906e-9ed18f482e93", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 49:======================================================> (38 + 1) / 39]\r"}, {"name": "stdout", "output_type": "stream", "text": "Number of Rows:     21779518 \nNumber of Columns:  726\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "print(\"Number of Rows:    \", CA_df.count(), \\\n      \"\\nNumber of Columns: \", len(CA_df.columns))"}, {"cell_type": "code", "execution_count": null, "id": "cd68b9e1-6ce1-4881-b1c2-367be132ce6b", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.15"}}, "nbformat": 4, "nbformat_minor": 5}